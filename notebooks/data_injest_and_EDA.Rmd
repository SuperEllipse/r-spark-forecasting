```{r}
library(sparklyr)
library(dplyr)
library(lubridate)

# Spark configuration
config <- spark_config()
config$spark.security.credentials.hiveserver2.enabled <- "false"
config$spark.datasource.hive.warehouse.read.via.llap <- "false"
config$spark.sql.hive.hwc.execution.mode <- "spark"
config$spark.sql.extensions <- "com.qubole.spark.hiveacid.HiveAcidAutoConvertExtension"
config$spark.kryo.registrator <- "com.qubole.spark.hiveacid.util.HiveAcidKyroRegistrator"
config$sparklyr.jars.default <- "/opt/spark/optional-lib/hive-warehouse-connector-assembly.jar"
config$spark.yarn.access.hadoopFileSystems <- "s3a://go01-demo/datalake/warehouse/tablespace/managed/hive"

# Connect to Spark
sc <- spark_connect(config = config)
```


```{r}
# Specify S3 bucket and file path
#s3_path <- "s3a://go01-demo/data/iowa-liquor-sales/Iowa_Liquor_Sales.csv"
s3_path <- file.path(Sys.getenv("DATASET_PATH"),Sys.getenv( "DATASET_NAME"))

# Read the dataset from S3 into Spark
sales_tbl <- spark_read_csv(sc, name = "sales_data", path = s3_path, header = TRUE, infer_schema = TRUE)

# Rename columns to lowercase
sales_data <- sales_tbl %>%
  rename_with(tolower)

# EDA - Get summary stats
distinct_stores <- sales_data %>%
  select(store_number) %>%
  distinct() %>%
  collect()

# Aggregate by store and date, summing sale_dollars
aggregated_df <- sales_data %>%
  group_by(store_number, date) %>%
  summarize(total_sales = sum(sale_dollars), .groups = 'drop')

print(class(aggregated_df))  # Check class of aggregated_df

# Collect the results back to R
aggregated_result <- collect(aggregated_df)
```
```{r}
head(aggregated_result)
```


```{r}
# This is our base data 
head(sales_data)
```
```{r}
# let us get number of rows for each
# Perform the aggregation
result <- sales_data %>%
  mutate(
    date_parsed = to_date(date, "MM/dd/yyyy"),
    year = year(date_parsed)
  ) %>%
  group_by(year) %>%
  summarize(
    min_date = min(date_parsed, na.rm = TRUE),
    max_date = max(date_parsed, na.rm = TRUE),
    row_count = n()
  ) %>%
  arrange(desc(year))

result_df = collect(result)
result_df
```


Since the dataset is quite large. Let us use only last 3 years from 2019 to 2022 and save this smaller dataset in our S3 bucket, in addition let us add new columns for transformation that includes day of the month, month, week , year as numbers. 

```{r}
most_recent_date <- sales_data %>%
  summarize(max_date = max(to_date(date, "MM/dd/yyyy"))) %>%
  collect() %>%
  pull(max_date)

cutoff_date <- most_recent_date - years(3)

last_3_years_data <- sales_data %>%
  filter(to_date(date, "MM/dd/yyyy") >= cutoff_date) %>%
  mutate(
    date_parsed = to_date(date, "MM/dd/yyyy"),
    year = year(date_parsed),
    month = month(date_parsed),
    day_of_month = dayofmonth(date_parsed),
    week_number = weekofyear(date_parsed),
    day_of_week = dayofweek(date_parsed),
    quarter = quarter(date_parsed),
    is_weekend = (dayofweek(date_parsed) %in% c(1, 7))
  )
```

Let us save these results as a separate dataset, which we will use for Forecasting 
```{r}

spark_write_csv(last_3_years_data, path = "s3a://go01-demo/data/iowa-liquor-sales/Iowa_Liquor_Sales_3yrs.csv",  mode = "overwrite")
```

